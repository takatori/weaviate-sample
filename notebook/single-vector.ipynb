{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Single-Vector è©•ä¾¡\n",
    "\n",
    "é€šå¸¸ã®ã‚·ãƒ³ã‚°ãƒ«ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã§JQaRAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è©•ä¾¡ã—ã¾ã™ã€‚\n",
    "\n",
    "- **ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**: JQaRA (Japanese Question-Answer Retrieval Assessment)\n",
    "- **ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°**: sentence-transformers (åˆ‡ã‚Šæ›¿ãˆå¯èƒ½)\n",
    "- **è©•ä¾¡æ–¹å¼**: å„ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦100ä»¶ã®å€™è£œæ–‡æ›¸ã‚’ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section1-md",
   "metadata": {},
   "source": [
    "## 1. ãƒ¢ãƒ‡ãƒ«é¸æŠã¨Weaviateæ¥ç¶š\n",
    "\n",
    "ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "section1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: cl-nagoya/ruri-v3-310m\n",
      "Collection name: jqara_single_ruri_v3_310\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "# ---------- ãƒ¢ãƒ‡ãƒ«é¸æŠ ----------\n",
    "SUPPORTED_MODELS = {\n",
    "    \"multilingual-e5-small\": \"intfloat/multilingual-e5-small\",      # 384æ¬¡å…ƒ\n",
    "    \"multilingual-e5-base\": \"intfloat/multilingual-e5-base\",        # 768æ¬¡å…ƒ\n",
    "    \"multilingual-e5-large\": \"intfloat/multilingual-e5-large\",      # 1024æ¬¡å…ƒ\n",
    "    \"ruri-v3-310\": \"cl-nagoya/ruri-v3-310m\",\n",
    "}\n",
    "\n",
    "# ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆ\n",
    "# SELECTED_MODEL = \"multilingual-e5-small\"\n",
    "SELECTED_MODEL = \"ruri-v3-310\"\n",
    "\n",
    "MODEL_NAME = SUPPORTED_MODELS[SELECTED_MODEL]\n",
    "print(f\"Using model: {MODEL_NAME}\")\n",
    "\n",
    "# Weaviateæ¥ç¶š\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "# ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åï¼ˆãƒ¢ãƒ‡ãƒ«ã”ã¨ã«åˆ†ã‘ã‚‹ï¼‰\n",
    "COLLECTION_NAME = f\"jqara_single_{SELECTED_MODEL.replace('-', '_').replace('.', '_')}\"\n",
    "print(f\"Collection name: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-md",
   "metadata": {},
   "source": [
    "## 2. ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿\n",
    "\n",
    "sentence-transformersã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "section2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: cl-nagoya/ruri-v3-310m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The checkpoint you are trying to load has model type `modernbert` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1034\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1033\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m     config_class = \u001b[43mCONFIG_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:736\u001b[39m, in \u001b[36m_LazyConfigMapping.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mapping:\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    737\u001b[39m value = \u001b[38;5;28mself\u001b[39m._mapping[key]\n",
      "\u001b[31mKeyError\u001b[39m: 'modernbert'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.get_sentence_embedding_dimension()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:327\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    309\u001b[39m has_modules = is_sentence_transformer_model(\n\u001b[32m    310\u001b[39m     model_name_or_path,\n\u001b[32m    311\u001b[39m     token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    314\u001b[39m     local_files_only=local_files_only,\n\u001b[32m    315\u001b[39m )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    317\u001b[39m     has_modules\n\u001b[32m    318\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_model_type(\n\u001b[32m   (...)\u001b[39m\u001b[32m    325\u001b[39m     == \u001b[38;5;28mself\u001b[39m._model_config[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    326\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    339\u001b[39m     modules = \u001b[38;5;28mself\u001b[39m._load_auto_model(\n\u001b[32m    340\u001b[39m         model_name_or_path,\n\u001b[32m    341\u001b[39m         token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m         has_modules=has_modules,\n\u001b[32m    350\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:2305\u001b[39m, in \u001b[36mSentenceTransformer._load_sbert_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[39m\n\u001b[32m   2300\u001b[39m         module = module_class.load(local_path)\n\u001b[32m   2302\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2303\u001b[39m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[32m   2304\u001b[39m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2305\u001b[39m     module = \u001b[43mmodule_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2307\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Loading-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2308\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodule_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2313\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Module-specific keyword arguments\u001b[39;49;00m\n\u001b[32m   2314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2321\u001b[39m modules[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module\n\u001b[32m   2322\u001b[39m module_kwargs[module_config[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]] = module_config.get(\u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m, [])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:365\u001b[39m, in \u001b[36mTransformer.load\u001b[39m\u001b[34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m    336\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m     **kwargs,\n\u001b[32m    351\u001b[39m ) -> Self:\n\u001b[32m    352\u001b[39m     init_kwargs = \u001b[38;5;28mcls\u001b[39m._load_init_kwargs(\n\u001b[32m    353\u001b[39m         model_name_or_path=model_name_or_path,\n\u001b[32m    354\u001b[39m         subfolder=subfolder,\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m         backend=backend,\n\u001b[32m    364\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:87\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m config, is_peft_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m._load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Get the signature of the auto_model's forward method to pass only the expected arguments from `features`,\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# plus some common values like \"input_ids\", \"attention_mask\", etc.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:162\u001b[39m, in \u001b[36mTransformer._load_config\u001b[39m\u001b[34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[39m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1036\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1034\u001b[39m         config_class = CONFIG_MAPPING[config_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1037\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe checkpoint you are trying to load has model type `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[33m'\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1038\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mbut Transformers does not recognize this architecture. This could be because of an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1039\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33missue with the checkpoint, or because your version of Transformers is out of date.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1040\u001b[39m         )\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m config_class.from_dict(config_dict, **unused_kwargs)\n\u001b[32m   1042\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1043\u001b[39m     \u001b[38;5;66;03m# Fallback: use pattern matching on the string.\u001b[39;00m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;66;03m# We go from longer names to shorter names to catch roberta before bert (for instance)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: The checkpoint you are trying to load has model type `modernbert` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "print(f\"Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-md",
   "metadata": {},
   "source": [
    "## 3. ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆ\n",
    "\n",
    "ã‚·ãƒ³ã‚°ãƒ«ãƒ™ã‚¯ãƒˆãƒ«ç”¨ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¾ã™ã€‚\n",
    "- `q_id`: ã‚¯ã‚¨ãƒªIDã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ‰åŠ¹ï¼‰\n",
    "- `vector`: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "section3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satoshi.takatori.001/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/weaviate/warnings.py:196: DeprecationWarning: Dep024: You are using the `vectorizer_config` argument in `collection.config.create()`, which is deprecated.\n",
      "            Use the `vector_config` argument instead.\n",
      "            \n",
      "  warnings.warn(\n",
      "/Users/satoshi.takatori.001/Desktop/playground/weaviate-sample/.venv/lib/python3.12/site-packages/weaviate/warnings.py:206: DeprecationWarning: Dep025: You are using the `vector_index_config` argument in `collection.config.create()`, which is deprecated.\n",
      "            Use the `vector_config` argument instead defining `vector_index_config` as a sub-argument.\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection created: jqara_single_multilingual_e5_small\n"
     ]
    }
   ],
   "source": [
    "from weaviate.classes.config import Configure, Property, DataType, VectorDistances\n",
    "\n",
    "if client.collections.exists(COLLECTION_NAME):\n",
    "    client.collections.delete(COLLECTION_NAME)\n",
    "\n",
    "client.collections.create(\n",
    "    name=COLLECTION_NAME,\n",
    "    vectorizer_config=Configure.Vectorizer.none(),\n",
    "    vector_index_config=Configure.VectorIndex.hnsw(\n",
    "        distance_metric=VectorDistances.COSINE\n",
    "    ),\n",
    "    properties=[\n",
    "        Property(name=\"d_id\", data_type=DataType.TEXT),\n",
    "        Property(name=\"q_id\", data_type=DataType.TEXT, index_filterable=True),\n",
    "        Property(name=\"title\", data_type=DataType.TEXT),\n",
    "        Property(name=\"text\", data_type=DataType.TEXT),\n",
    "    ],\n",
    ")\n",
    "\n",
    "collection = client.collections.get(COLLECTION_NAME)\n",
    "print(f\"Collection created: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-md",
   "metadata": {},
   "source": [
    "## 4. JQaRAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿\n",
    "\n",
    "JQaRAãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚å„ãƒ¬ã‚³ãƒ¼ãƒ‰ã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™ï¼š\n",
    "- `id`: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆID\n",
    "- `q_id`: ã‚¯ã‚¨ãƒªIDï¼ˆåŒä¸€ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹è¤‡æ•°ã®å€™è£œæ–‡æ›¸ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰\n",
    "- `label`: æ­£è§£ãƒ©ãƒ™ãƒ«ï¼ˆ1=é–¢é€£, 0=éé–¢é€£ï¼‰\n",
    "- `text`, `title`: æ–‡æ›¸å†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "section4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 166700 records\n",
      "{'id': 'QA20CAPR-1004#5191928', 'q_id': 'QA20CAPR-1004', 'passage_row_id': '5191928', 'label': 1, 'text': 'çµ¶å¯¾é›¶åº¦(ãœã£ãŸã„ã‚Œã„ã©ã€ã‚¢ãƒ–ã‚½ãƒªãƒ¥ãƒ¼ãƒˆã‚¼ãƒ­ã€è‹±: Absolute zero)ã¯ã€ç†±åŠ›å­¦ä¸Šã®æœ€ä½æ¸©åº¦ã§ã‚ã‚‹æ‘‚æ°âˆ’273.15åº¦ã€‚', 'title': 'çµ¶å¯¾é›¶åº¦ (æ›–æ˜§ã•å›é¿)', 'question': 'æ‘‚æ°ã§ã¯ãƒã‚¤ãƒŠã‚¹273.15åº¦ã«ã‚ãŸã‚‹ã€å…¨ã¦ã®åŸå­ã®æŒ¯å‹•ãŒåœæ­¢ã™ã‚‹æœ€ã‚‚ä½ã„æ¸©åº¦ã‚’ä½•ã¨ã„ã†ã§ã—ã‚‡ã†?', 'answers': ['çµ¶å¯¾é›¶åº¦']}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = []\n",
    "# notebookãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹\n",
    "project_root = Path(os.getcwd()).parent\n",
    "file_path = project_root / \"data\" / \"jqara_test.jsonl\"\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        obj = json.loads(line)        \n",
    "        dataset.append(obj)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} records\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-md",
   "metadata": {},
   "source": [
    "## 5. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆã¨ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "\n",
    "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ(title + text)ã‚’ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦Weaviateã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n",
    "\n",
    "E5ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯ `passage: ` ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ä»˜ã‘ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "section5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 166700 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2605/2605 [09:09<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (166700, 384)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "N = len(dataset)  # å…¨ä»¶å‡¦ç†\n",
    "subset = dataset[:N]\n",
    "\n",
    "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™ï¼ˆE5å½¢å¼: passage: prefixï¼‰\n",
    "def prepare_doc_text(title: str, text: str) -> str:\n",
    "    title = (title or \"\").strip()\n",
    "    text = (text or \"\").strip()\n",
    "    if title and text:\n",
    "        content = f\"{title}\\n\\n{text}\"\n",
    "    else:\n",
    "        content = title or text or \"\"\n",
    "    return f\"passage: {content}\"\n",
    "\n",
    "doc_texts = [prepare_doc_text(obj[\"title\"], obj[\"text\"]) for obj in subset]\n",
    "\n",
    "print(f\"Encoding {len(doc_texts)} documents...\")\n",
    "doc_embeddings = model.encode(doc_texts, show_progress_bar=True, batch_size=64)\n",
    "print(f\"Embeddings shape: {doc_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "section5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ Uploading to Weaviate: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 166700/166700 [00:50<00:00, 3313.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All objects uploaded successfully!\n",
      "ğŸ§® ç¾åœ¨ã®ç™»éŒ²ä»¶æ•°: 166700\n"
     ]
    }
   ],
   "source": [
    "# Weaviateã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\n",
    "total = len(subset)\n",
    "\n",
    "with collection.batch.fixed_size(batch_size=100) as batch, tqdm(total=total, desc=\"ğŸ“¤ Uploading to Weaviate\") as pbar:\n",
    "    for obj, emb in zip(subset, doc_embeddings):\n",
    "        batch.add_object(\n",
    "            properties={\n",
    "                \"d_id\": obj[\"id\"],\n",
    "                \"q_id\": obj[\"q_id\"],\n",
    "                \"text\": obj[\"text\"],\n",
    "                \"title\": obj[\"title\"],\n",
    "            },\n",
    "            vector=emb.tolist(),\n",
    "        )\n",
    "        pbar.update(1)\n",
    "\n",
    "if collection.batch.failed_objects:\n",
    "    print(f\"\\nâŒ Number of failed imports: {len(collection.batch.failed_objects)}\")\n",
    "    print(f\"First failed object: {collection.batch.failed_objects[0]}\")\n",
    "else:\n",
    "    print(\"\\nâœ… All objects uploaded successfully!\")\n",
    "\n",
    "count = collection.aggregate.over_all(total_count=True).total_count\n",
    "print(f\"ğŸ§® ç¾åœ¨ã®ç™»éŒ²ä»¶æ•°: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-md",
   "metadata": {},
   "source": [
    "## 6. è©•ä¾¡ç”¨ãƒ‡ãƒ¼ã‚¿æ§‹é€ ä½œæˆ\n",
    "\n",
    "ranxè©•ä¾¡ç”¨ã®qrelsï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ï¼‰ã¨qid_to_queryï¼ˆã‚¯ã‚¨ãƒªãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "section6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 166700 records\n",
      "Unique queries: 1667\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "qid_to_query: Dict[str, str] = {}\n",
    "qrels: Dict[str, Dict[str, int]] = {}\n",
    "\n",
    "cnt = 0\n",
    "for rec in subset:\n",
    "    cnt += 1\n",
    "    qid   = rec[\"q_id\"]\n",
    "    q = rec.get(\"question\")\n",
    "    docid = rec.get(\"id\")\n",
    "    rel   = int(rec.get(\"label\", 0))\n",
    "\n",
    "    if qid not in qid_to_query:\n",
    "        qid_to_query[qid] = q\n",
    "    qrels.setdefault(qid, {})[docid] = rel\n",
    "\n",
    "print(f\"Loaded {cnt} records\")\n",
    "print(f\"Unique queries: {len(qid_to_query)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-md",
   "metadata": {},
   "source": [
    "## 7. éåŒæœŸæ¤œç´¢ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°\n",
    "\n",
    "å…¨ã‚¯ã‚¨ãƒªã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã¦run_dictï¼ˆæ¤œç´¢çµæœï¼‰ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’åé›†ã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "section7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Awaitable\n",
    "import asyncio\n",
    "\n",
    "async def build_run_dict_async_with_qid(\n",
    "        search_fn: Callable[[str, str, int], Awaitable[Tuple[List[Tuple[str, float]], float]]],\n",
    "        qid_to_query: Dict[str, str],\n",
    "        topk: int = 100\n",
    "    ) -> Tuple[Dict[str, Dict[str, float]], List[float]]:\n",
    "    \"\"\"\n",
    "    éåŒæœŸæ¤œç´¢é–¢æ•°ã‚’ä½¿ã£ã¦å…¨ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã€run_dictã¨latenciesã‚’è¿”ã™ã€‚\n",
    "    q_idã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‚\n",
    "    \n",
    "    Args:\n",
    "        search_fn: (q, q_id, topk) -> ([(doc_id, score), ...], elapsed)\n",
    "        qid_to_query: {qid: query_text} ã®è¾æ›¸\n",
    "        topk: å„ã‚¯ã‚¨ãƒªã§å–å¾—ã™ã‚‹ä»¶æ•°\n",
    "        \n",
    "    Returns:\n",
    "        run_dict: {qid: {doc_id: score, ...}, ...}\n",
    "        latencies: å„ã‚¯ã‚¨ãƒªã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    CONCURRENCY = 10\n",
    "    sem = asyncio.Semaphore(CONCURRENCY)    \n",
    "    run_dict: Dict[str, Dict[str, float]] = {}\n",
    "    latencies: List[float] = []\n",
    "\n",
    "    async def _one(qid: str, q: str):\n",
    "        async with sem:\n",
    "            ranked, elapsed = await search_fn(q, qid, topk)\n",
    "            return qid, ranked, elapsed\n",
    "\n",
    "    tasks = [_one(qid, q) for qid, q in qid_to_query.items()]\n",
    "    \n",
    "    for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Searching\"):\n",
    "        qid, ranked, elapsed = await coro\n",
    "        run_dict[qid] = {doc_id: score for doc_id, score in ranked}\n",
    "        latencies.append(elapsed)\n",
    "    \n",
    "    return run_dict, latencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-md",
   "metadata": {},
   "source": [
    "## 8. ã‚·ãƒ³ã‚°ãƒ«ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢é–¢æ•°\n",
    "\n",
    "q_idã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã‚·ãƒ³ã‚°ãƒ«ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
    "- ã‚¯ã‚¨ãƒªã‚’ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆE5å½¢å¼: `query: ` prefixï¼‰\n",
    "- è©²å½“q_idã®å€™è£œæ–‡æ›¸ç¾¤å†…ã§near_vectoræ¤œç´¢\n",
    "- distanceã‚’åè»¢ã—ã¦ã‚¹ã‚³ã‚¢åŒ–ï¼ˆranxã¯å¤§ãã„ã‚¹ã‚³ã‚¢ã‚’ä¸Šä½ã¨ã™ã‚‹ãŸã‚ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "section8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import MetadataQuery, Filter\n",
    "import time\n",
    "\n",
    "async def async_single_vector_search(async_col, q: str, q_id: str, topk: int = 100) -> Tuple[List[Tuple[str, float]], float]:\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ³ã‚°ãƒ«ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”¨ã„ãŸéåŒæœŸæ¤œç´¢é–¢æ•°\n",
    "    q_idã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã€è©²å½“ã‚¯ã‚¨ãƒªã®å€™è£œæ–‡æ›¸ç¾¤ã®ä¸­ã‹ã‚‰ãƒ©ãƒ³ã‚­ãƒ³ã‚°\n",
    "    \n",
    "    Args:\n",
    "        async_col: éåŒæœŸWeaviateã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³\n",
    "        q: æ¤œç´¢ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—\n",
    "        q_id: ãƒ•ã‚£ãƒ«ã‚¿ç”¨ã®ã‚¯ã‚¨ãƒªID\n",
    "        topk: å–å¾—ã™ã‚‹ä»¶æ•°\n",
    "        \n",
    "    Returns:\n",
    "        ([(doc_id, score), ...], elapsed) ã®ã‚¿ãƒ—ãƒ«\n",
    "        scoreã¯å¤§ãã„ã»ã©è‰¯ã„ï¼ˆdistanceã‚’åè»¢ï¼‰\n",
    "    \"\"\"\n",
    "    # ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼ˆE5å½¢å¼: query: prefixï¼‰\n",
    "    q_text = f\"query: {q}\"\n",
    "    q_vec = model.encode(q_text).tolist()\n",
    "\n",
    "    # éåŒæœŸã‚¯ã‚¨ãƒªã®å®Ÿè¡Œï¼ˆq_idã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    response = await async_col.query.near_vector(\n",
    "        near_vector=q_vec,\n",
    "        limit=topk,\n",
    "        filters=Filter.by_property(\"q_id\").equal(q_id),\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "\n",
    "    elapsed = time.perf_counter() - start\n",
    "\n",
    "    # çµæœã®æ•´å½¢: (doc_id, score) ã®ãƒªã‚¹ãƒˆ\n",
    "    # ranxã¯å¤§ãã„ã‚¹ã‚³ã‚¢ã‚’ä¸Šä½ã¨ã™ã‚‹ãŸã‚ã€distanceã‚’åè»¢ï¼ˆè² ã«ã™ã‚‹ï¼‰\n",
    "    results: List[Tuple[str, float]] = []\n",
    "    for obj in response.objects:\n",
    "        doc_id = obj.properties.get(\"d_id\", \"\")\n",
    "        score = -obj.metadata.distance  # è·é›¢ã‚’åè»¢ï¼\n",
    "        results.append((doc_id, score))\n",
    "\n",
    "    return results, elapsed\n",
    "\n",
    "\n",
    "def create_search_fn(async_col):\n",
    "    \"\"\"async_colã‚’ãƒã‚¤ãƒ³ãƒ‰ã—ãŸæ¤œç´¢é–¢æ•°ã‚’è¿”ã™ãƒ•ã‚¡ã‚¯ãƒˆãƒªï¼ˆq_idå¯¾å¿œç‰ˆï¼‰\"\"\"\n",
    "    async def search_fn(q: str, q_id: str, topk: int = 100) -> Tuple[List[Tuple[str, float]], float]:\n",
    "        return await async_single_vector_search(async_col, q, q_id, topk)\n",
    "    return search_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9-md",
   "metadata": {},
   "source": [
    "## 9. å…¨ã‚¯ã‚¨ãƒªã§æ¤œç´¢å®Ÿè¡Œ\n",
    "\n",
    "å…¨ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦ã‚·ãƒ³ã‚°ãƒ«ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã‚’å®Ÿè¡Œã—ã€çµæœã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’åé›†ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "section9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1667/1667 [00:16<00:00, 102.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¤œç´¢å®Œäº†: 1667 ã‚¯ã‚¨ãƒª\n",
      "å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: 0.0697 ç§’\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "search_run_dict = {}\n",
    "search_latencies = []\n",
    "\n",
    "async with weaviate.use_async_with_local() as async_client:\n",
    "    async_col = async_client.collections.get(COLLECTION_NAME)\n",
    "    \n",
    "    # async_col ã‚’ãƒã‚¤ãƒ³ãƒ‰ã—ãŸæ¤œç´¢é–¢æ•°ã‚’ä½œæˆ\n",
    "    search_fn = create_search_fn(async_col)\n",
    "    \n",
    "    # å…¨ã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œï¼ˆq_idã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼‰\n",
    "    search_run_dict, search_latencies = await build_run_dict_async_with_qid(\n",
    "        search_fn, \n",
    "        qid_to_query,\n",
    "        topk=100\n",
    "    )\n",
    "\n",
    "print(f\"æ¤œç´¢å®Œäº†: {len(search_run_dict)} ã‚¯ã‚¨ãƒª\")\n",
    "print(f\"å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·: {sum(search_latencies) / len(search_latencies):.4f} ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section10-md",
   "metadata": {},
   "source": [
    "## 10. è©•ä¾¡é–¢æ•°å®šç¾©\n",
    "\n",
    "ranxã‚’ä½¿ç”¨ã—ã¦NDCG, MAP, MRR, Precision, Recallã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "section10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ranx import Qrels, Run, evaluate\n",
    "\n",
    "def evaluate_run_dict(run_dict: Dict[str, Dict[str, float]], qrels: Dict[str, Dict[str, int]]):\n",
    "    qrels_ranx = Qrels(qrels)\n",
    "    run_ranx   = Run(run_dict)\n",
    "\n",
    "    metrics = [\n",
    "        \"ndcg@1\", \"ndcg@3\", \"ndcg@5\", \"ndcg@10\",\n",
    "        \"map\", \"mrr\", \"mrr@10\", \"precision@10\", \"recall@10\", \"recall@100\",\n",
    "    ]\n",
    "\n",
    "    scores = evaluate(qrels_ranx, run_ranx, metrics=metrics)\n",
    "    for m in metrics:\n",
    "        print(f\"{m:>12}: {scores[m]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section11-md",
   "metadata": {},
   "source": [
    "## 11. ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·çµ±è¨ˆé–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "section11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def print_latency_stats(latencies: List[float]):\n",
    "    l = np.array(latencies)\n",
    "\n",
    "    print(f\"Count    : {len(l)}\")\n",
    "    print(f\"Mean     : {l.mean():.4f} sec\")\n",
    "    print(f\"Median   : {np.median(l):.4f} sec\")\n",
    "    print(f\"p90      : {np.percentile(l,90):.4f} sec\")\n",
    "    print(f\"p95      : {np.percentile(l,95):.4f} sec\")\n",
    "    print(f\"p99      : {np.percentile(l,99):.4f} sec\")\n",
    "    print(f\"Min/Max  : {l.min():.4f} / {l.max():.4f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section12-md",
   "metadata": {},
   "source": [
    "## 12. è©•ä¾¡çµæœ\n",
    "\n",
    "ã‚·ãƒ³ã‚°ãƒ«ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ç²¾åº¦ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "section12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== multilingual-e5-small ===\n",
      "Model: intfloat/multilingual-e5-small\n",
      "\n",
      "      ndcg@1: 0.6293\n",
      "      ndcg@3: 0.5417\n",
      "      ndcg@5: 0.5090\n",
      "     ndcg@10: 0.4914\n",
      "         map: 0.4284\n",
      "         mrr: 0.7317\n",
      "      mrr@10: 0.7283\n",
      "precision@10: 0.3312\n",
      "   recall@10: 0.4133\n",
      "  recall@100: 1.0000\n",
      "\n",
      "Count    : 1667\n",
      "Mean     : 0.0697 sec\n",
      "Median   : 0.0633 sec\n",
      "p90      : 0.0941 sec\n",
      "p95      : 0.1181 sec\n",
      "p99      : 0.2095 sec\n",
      "Min/Max  : 0.0124 / 0.7344 sec\n"
     ]
    }
   ],
   "source": [
    "print(f\"=== {SELECTED_MODEL} ===\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print()\n",
    "evaluate_run_dict(search_run_dict, qrels)\n",
    "print()\n",
    "print_latency_stats(search_latencies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
